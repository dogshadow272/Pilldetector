{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Renjian-buchai/buildingBloCS/blob/main/Intro_to_OpenCV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The AI of the Beholder: Seeing with Computer Vision ðŸ‘€\n",
        "This section provides an alternative way to running the code if you are unable to run it from downloading. Do **make a copy** of this Google Colab.\n",
        "\n",
        "We have compiled the code into sections. You can refer to the relevant sections for the codes we will be using."
      ],
      "metadata": {
        "id": "Kj9QI4WxJ7kG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section -1: Support and Help"
      ],
      "metadata": {
        "id": "Yv1A3G9d3XmG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Uploading Test Images\n",
        "If you would like to upload your own images for testing, you can do so by:\n",
        "1. Click on the files icon on the left sidebar<br>\n",
        "<img src=\"https://drive.google.com/uc?id=1iqFaTc1-IKEAEwQ0NFOA7BuEjUoALEJx\" alt=\"Files icon\" width=\"60px\">\n",
        "2. Expand the folder called `buildingblocs-opencv`. Click the 3 dots beside `img`.<br>\n",
        "<img src=\"https://drive.google.com/uc?id=1SUA1iXuWxbm5K14ssGVkt8xbIeF5s17s\" alt=\"Files display\" width=\"300px\">\n",
        "3. Press Upload and upload your image. You can then move it to any folder you want to.\n",
        "4. Access your image using `./img/...`, where ... is the filepath of the image inside the img folder. (e.g. if you added the image `test.png` into the `feature_detection` folder, the correct filepath is `./img/feature_detection/test.png`"
      ],
      "metadata": {
        "id": "sM9zw5t_3bSh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Support Links\n",
        "OpenCV tutorials: [https://docs.opencv.org/4.7.0/d9/df8/tutorial_root.html](https://docs.opencv.org/4.7.0/d9/df8/tutorial_root.html)  \n",
        "OpenCV documentation: [https://docs.opencv.org/4.7.0/](https://docs.opencv.org/4.7.0/)"
      ],
      "metadata": {
        "id": "GmReSahe3irV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 0: Installing and importing modules\n",
        "*Make sure to run, but do not modify this section!*"
      ],
      "metadata": {
        "id": "DNv1151BKUq2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7-1_q8raIBtu",
        "outputId": "ab48321d-7718-4c7f-be8b-3c323ea1f557",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'buildingblocs-opencv'...\n",
            "remote: Enumerating objects: 121, done.\u001b[K\n",
            "remote: Counting objects: 100% (121/121), done.\u001b[K\n",
            "remote: Compressing objects: 100% (88/88), done.\u001b[K\n",
            "remote: Total 121 (delta 47), reused 102 (delta 29), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (121/121), 4.30 MiB | 9.97 MiB/s, done.\n",
            "Resolving deltas: 100% (47/47), done.\n",
            "/content/buildingblocs-opencv\n"
          ]
        }
      ],
      "source": [
        "# run this block (but do NOT modify this block!)\n",
        "!git clone https://github.com/sawzedong/buildingblocs-opencv.git\n",
        "%cd buildingblocs-opencv\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from google.colab.patches import cv2_imshow"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 1: Image Processing"
      ],
      "metadata": {
        "id": "ccVbn-iLK8wk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding Images"
      ],
      "metadata": {
        "id": "uko9wyCnbyQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2 \n",
        "\n",
        "# Opening up image files\n",
        "img = cv2.imread(\"./img/image_processing/orchid.png\")\n",
        "\n",
        "# Shape of an image. It returns a tuple of the number of rows, columns, an channels\n",
        "# print(img.shape)\n",
        "\n",
        "# Finding specific pixels\n",
        "print(img[200, 150])\n",
        "\n",
        "# Cropping\n",
        "start_row, end_row = 0, 200\n",
        "start_col, end_col = 0, 100\n",
        "cropped = img[start_col:end_col, start_row:end_row]\n",
        "\n",
        "cv2_imshow(cropped)"
      ],
      "metadata": {
        "id": "ACf4J-3GK7ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformation"
      ],
      "metadata": {
        "id": "hXUmy8rOu8W4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2 \n",
        "import time \n",
        "\n",
        "# Opening up image files\n",
        "img = cv2.imread(\"./img/image_processing/orchid.png\")\n",
        "print('original')\n",
        "cv2_imshow(img)\n",
        "\n",
        "##########################################\n",
        "# Image Transformation\n",
        "\n",
        "#1. Resizing\n",
        "\n",
        "# Method 1 - Specifying width and height\n",
        "width = 300\n",
        "height = 300\n",
        "resized_img = cv2.resize(img, (width, height))\n",
        "print('resized')\n",
        "cv2_imshow(resized_img)\n",
        "\n",
        "# Method 2 - Scale Factor \n",
        "scale_x = 1.2\n",
        "scale_y = 1.2\n",
        "resizefactor_img = cv2.resize(img, None, fx=scale_x, fy=scale_y)\n",
        "print('scaleFactor')\n",
        "cv2_imshow(resizefactor_img)\n",
        "\n",
        "# 2. Cropping \n",
        "start_row, start_col = 100, 100\n",
        "end_row, end_col = 300, 300\n",
        "\n",
        "cropped = img[start_row:end_row, start_col:end_col]\n",
        "print(\"cropped\")\n",
        "cv2_imshow(cropped)"
      ],
      "metadata": {
        "id": "n9AKl0j1u-hF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Smoothing"
      ],
      "metadata": {
        "id": "mIbrZPuHu-mX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2 \n",
        "import time \n",
        "import numpy as np\n",
        "\n",
        "# Opening up image files\n",
        "img = cv2.imread(\"./img/image_processing/orchid.png\")\n",
        "\n",
        "##########################################\n",
        "# Image smoothing\n",
        "\n",
        "# Blurring Images\n",
        "kernel = (100, 100) # Experiment with these values\n",
        "img_blur = cv2.blur(img, kernel)\n",
        "print(\"Blur\")\n",
        "cv2_imshow(img_blur)\n",
        "\n",
        "# Image enhancement\n",
        "kernel = np.array([\n",
        "    [0, -1, 0],\n",
        "    [-1, 5, -1],\n",
        "    [0, -1, 0]\n",
        "])\n",
        "sharpened_image = cv2.filter2D(img, -1, kernel)\n",
        "print(\"Sharpened\")\n",
        "cv2_imshow(sharpened_image)"
      ],
      "metadata": {
        "id": "TUEhg0DDu_iC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Recolour"
      ],
      "metadata": {
        "id": "qI3yJkcOu_p4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Opening up image files\n",
        "img = cv2.imread(\"./img/image_processing/orchid.png\")\n",
        "\n",
        "##########################################\n",
        "# Image Recolouring\n",
        "\n",
        "#1. Recolour \n",
        "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Exploring Other Colour Spaces\n",
        "#   1. cv2.COLOR_BGR2HSV\n",
        "#   2. cv2.COLOR_BGR2Lab\n",
        "#   3. cv2.COLOR_BGR2RGB\n",
        "\n",
        "print(\"Recoloured\")\n",
        "cv2_imshow(gray)\n",
        "\n",
        "##########################################\n",
        "# Image Masking\n",
        "\n",
        "# Image Mask\n",
        "hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "# define range of pink cololur in HSV\n",
        "lower_pink = np.array([120, 0, 0])\n",
        "upper_pink = np.array([255, 255, 255])\n",
        "\n",
        "# create a mask\n",
        "mask = cv2.inRange(hsv, lower_pink, upper_pink)\n",
        "\n",
        "# Bitwise and mask\n",
        "result = cv2.bitwise_and(img, img, mask=mask)\n",
        "\n",
        "print(\"Mask\")\n",
        "cv2_imshow(result)"
      ],
      "metadata": {
        "id": "HCPpgfPEvBRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Histograms"
      ],
      "metadata": {
        "id": "p1ESpuRSvBYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "img = cv2.imread(\"./img/image_processing/orchid.png\")\n",
        "\n",
        "##########################################\n",
        "# Plotting Histograms\n",
        "\n",
        "colour = (\"b\", \"g\", \"r\")\n",
        "for i, col in enumerate(colour):\n",
        "    histr = cv2.calcHist(img, [i], None, [256], [0, 256])\n",
        "    plt.plot(histr, color=col)\n",
        "    plt.xlim([0, 256])\n",
        "\n",
        "plt.show()\n",
        "\n",
        "##########################################\n",
        "# Image Recolouring\n",
        "\n",
        "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "equ = cv2.equalizeHist(gray)\n",
        "res = np.hstack((gray, equ))\n",
        "print(\"Transformed\")\n",
        "cv2_imshow(res)"
      ],
      "metadata": {
        "id": "BXGvpQtDvD0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 2A: Feature Extraction\n",
        "\n",
        "- OpenCV Feature Detectors: [https://docs.opencv.org/4.x/db/d27/tutorial_py_table_of_contents_feature2d.html](https://docs.opencv.org/4.x/db/d27/tutorial_py_table_of_contents_feature2d.html)\n",
        "- Harris Corner Detection: [https://docs.opencv.org/4.x/dd/d1a/group__imgproc__feature.html#gac1fc3598018010880e370e2f709b4345](https://docs.opencv.org/4.x/dd/d1a/group__imgproc__feature.html#gac1fc3598018010880e370e2f709b4345)\n",
        "- Shi-Tomasi Corner Detection: [https://docs.opencv.org/4.x/dd/d1a/group__imgproc__feature.html#ga1d6bb77486c8f92d79c8793ad995d541](https://docs.opencv.org/4.x/dd/d1a/group__imgproc__feature.html#ga1d6bb77486c8f92d79c8793ad995d541)\n",
        "- Canny Edge Detection: [https://docs.opencv.org/3.4/da/d22/tutorial_py_canny.html](https://docs.opencv.org/3.4/da/d22/tutorial_py_canny.html)\n",
        "- Simple Blob Detector: [https://docs.opencv.org/4.x/d0/d7a/classcv_1_1SimpleBlobDetector.html](https://docs.opencv.org/4.x/d0/d7a/classcv_1_1SimpleBlobDetector.html)"
      ],
      "metadata": {
        "id": "F0h2uB2oLCYy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Harris Corner Detection"
      ],
      "metadata": {
        "id": "JDIaIsgNSrlI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2 \n",
        "img = cv2.imread(\"./img/feature_detection/chessboard.png\")\n",
        "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "gray = np.float32(gray)\n",
        "\n",
        "# apply corner detection\n",
        "dst = cv2.cornerHarris(gray, 2, 3, 0.04)\n",
        "dst = cv2.dilate(dst, None) # dilation for marking the corners\n",
        "img[dst>0.01*dst.max()]=[0,0,255] # revert back w/ optimal threshold\n",
        "cv2_imshow(img)"
      ],
      "metadata": {
        "id": "QY4MznfeLFnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Shi-Tomasi Corner Detection"
      ],
      "metadata": {
        "id": "-CkSfNRfSvVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2 \n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "img = cv2.imread(\"./img/feature_detection/blocks.png\")\n",
        "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# detect corners\n",
        "corners = cv2.goodFeaturesToTrack(gray, 50, 0.01, 10)\n",
        "corners = np.intp(corners)\n",
        "\n",
        "# mark corners\n",
        "for i in corners:\n",
        "    x, y = i.ravel() \n",
        "    cv2.circle(img, (x,y), 15, 255, -1)\n",
        "\n",
        "plt.imshow(img), plt.show()"
      ],
      "metadata": {
        "id": "fsGC1pMvSuAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Canny Edge Detection"
      ],
      "metadata": {
        "id": "e95t-6YOS17v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2 \n",
        "\n",
        "img = cv2.imread(\"./img/feature_detection/street.png\")\n",
        "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# detect edges\n",
        "edges = cv2.Canny(gray, 100, 200)\n",
        "cv2_imshow(edges)"
      ],
      "metadata": {
        "id": "nsC4teopS8LR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple Blob Detection"
      ],
      "metadata": {
        "id": "bwZ49tmfTDKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np;\n",
        " \n",
        " \n",
        "# set up the detector with default parameters.\n",
        "params = cv2.SimpleBlobDetector_Params()\n",
        "\n",
        "# feel free to change the following parameters at your own discretion!\n",
        "\n",
        "# threshold parameters\n",
        "params.minThreshold = 10;\n",
        "params.maxThreshold = 200;\n",
        "\n",
        "# area parameters\n",
        "params.filterByArea = True;\n",
        "params.minArea = 1500;\n",
        "\n",
        "# circularity parameters\n",
        "params.filterByCircularity = True;\n",
        "params.minCircularity = 0.1\n",
        "\n",
        "# convexity parameters\n",
        "params.filterByConvexity = True;\n",
        "params.minConvexity = 0.87;\n",
        "\n",
        "# inertia parameters\n",
        "params.filterByInertia = True;\n",
        "params.minInertiaRatio = 0.01\n",
        "\n",
        "# detect & mark blobs\n",
        "image = cv2.imread(\"./img/feature_detection/blob.png\", cv2.IMREAD_GRAYSCALE)\n",
        "detector = cv2.SimpleBlobDetector_create(params)\n",
        "keypoints = detector.detect(image)\n",
        "img_with_kps = cv2.drawKeypoints(image, keypoints, np.array([]), (0,0,255), cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
        "\n",
        "# show result\n",
        "cv2_imshow(img_with_kps)"
      ],
      "metadata": {
        "id": "iYvjhQMcTC0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 2B: Feature Matching\n",
        "- OpenCV Tutorial: https://docs.opencv.org/4.x/dc/dc3/tutorial_py_matcher.html"
      ],
      "metadata": {
        "id": "7uc3cJJkwIeZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ORB Corner Detection"
      ],
      "metadata": {
        "id": "GcUac22Gwk7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "import cv2 \n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "img = cv2.imread(\"./img/feature_detection/pineapple.png\")\n",
        "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "orb = cv2.ORB_create()\n",
        "keypoints = orb.detect(img, None)\n",
        "keypoints, descs = orb.compute(img, keypoints)\n",
        "\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "for kp in keypoints:\n",
        "    x, y = np.intp(kp.pt[0]), np.intp(kp.pt[1])\n",
        "    cv2.circle(img, (x, y), 4, 255, -1) # image, (x_coord, y_coord), size, color, filled/unfilled\n",
        "\n",
        "plt.imshow(img), plt.show()"
      ],
      "metadata": {
        "id": "kL71e4Avwkjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BFMatcher"
      ],
      "metadata": {
        "id": "pJRmU9A3wlSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2 \n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "obj_img = cv2.imread('./img/feature_detection/pineapple.png', cv2.IMREAD_GRAYSCALE)\n",
        "scene_img = cv2.imread('./img/feature_detection/fruit_pile.png', cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "orb = cv2.ORB_create()\n",
        "kp1, des1 = orb.detectAndCompute(obj_img, None)\n",
        "kp2, des2 = orb.detectAndCompute(scene_img, None)\n",
        "\n",
        "# create Brute Force Matcher\n",
        "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
        "\n",
        "# match descriptors...\n",
        "matches = bf.match(des1, des2)\n",
        "\n",
        "# ...and then sort them\n",
        "matches = sorted(matches, key = lambda x: x.distance)\n",
        "\n",
        "res = cv2.drawMatches(obj_img, kp1, scene_img, kp2, matches[:10], None)\n",
        "\n",
        "plt.imshow(res), plt.show()"
      ],
      "metadata": {
        "id": "VmJXnNlQwU_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FLANN-based Matcher"
      ],
      "metadata": {
        "id": "8BxpVEP4w99v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2 \n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "obj_img = cv2.imread('./img/feature_detection/pineapple.png', cv2.IMREAD_GRAYSCALE)\n",
        "scene_img = cv2.imread('./img/feature_detection/fruit_pile.png', cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "orb = cv2.ORB_create()\n",
        "kp1, des1 = orb.detectAndCompute(obj_img, None)\n",
        "kp2, des2 = orb.detectAndCompute(scene_img, None)\n",
        "\n",
        "FLANN_INDEX_LSH = 6\n",
        "index_params = dict(\n",
        "    algorithm = FLANN_INDEX_LSH,\n",
        "    table_number = 6,\n",
        "    key_size = 12,\n",
        "    multi_probe_level = 1\n",
        ")\n",
        "search_params = dict(checks = 100)\n",
        "\n",
        "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
        "matches = flann.knnMatch(des1, des2, k=2)\n",
        "\n",
        "matchesMask = [[0, 0] for i in range(len(matches))]\n",
        "\n",
        "\n",
        "for i,tup in enumerate(matches):\n",
        "    if len(tup) == 2:\n",
        "        m, n = tup\n",
        "        if m.distance < 0.7 * n.distance:\n",
        "            matchesMask[i] = [1, 0]\n",
        "\n",
        "draw_params = dict(\n",
        "    matchesMask = matchesMask,\n",
        ")\n",
        "\n",
        "res = cv2.drawMatchesKnn(obj_img, kp1, scene_img, kp2, matches, None, **draw_params)\n",
        "\n",
        "plt.imshow(res), plt.show()"
      ],
      "metadata": {
        "id": "bK3vUFU4xCK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 3A: Haar-cascade Object Detection\n",
        "\n",
        "- OpenCV Tutorial: [https://docs.opencv.org/4.7.0/db/d28/tutorial_cascade_classifier.html](https://docs.opencv.org/4.7.0/db/d28/tutorial_cascade_classifier.html)\n",
        "- OpenCV Pre-Trained Cascades: [https://github.com/opencv/opencv/tree/4.7.0/data/haarcascades](https://github.com/opencv/opencv/tree/4.7.0/data/haarcascades)"
      ],
      "metadata": {
        "id": "vSpb_yf4LGGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "# set up face cascade: initialise and load the cascade\n",
        "face_cascade_name = os.path.join(cv2.data.haarcascades,'haarcascade_frontalface_alt.xml')\n",
        "face_cascade = cv2.CascadeClassifier()\n",
        "face_cascade.load(cv2.samples.findFile(face_cascade_name))\n",
        "\n",
        "# load and process image\n",
        "frame = cv2.imread('./img/object_detection/people1.jpg')\n",
        "frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "frame_gray = cv2.equalizeHist(frame_gray)\n",
        "# NOTE: we apply the grayscale and histogram equalisation on a separate frame, so we can still display the original frame later\n",
        "\n",
        "# detect and plot faces\n",
        "faces = face_cascade.detectMultiScale(frame_gray)\n",
        "for (x, y, w, h) in faces:\n",
        "    frame = cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 255), 4)\n",
        "\n",
        "# show results\n",
        "cv2_imshow(frame)"
      ],
      "metadata": {
        "id": "ejrJ_N_dLP5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 3B: DNN-based Face Detection\n",
        "\n",
        "- OpenCV Tutorial: [https://docs.opencv.org/4.x/d0/dd4/tutorial_dnn_face.html](https://docs.opencv.org/4.x/d0/dd4/tutorial_dnn_face.html)\n",
        "- FaceDetectorYN Documentation: [https://docs.opencv.org/4.x/df/d20/classcv_1_1FaceDetectorYN.html](https://docs.opencv.org/4.x/df/d20/classcv_1_1FaceDetectorYN.html)\n",
        "- Model Download Link: [https://github.com/opencv/opencv_zoo/tree/master/models/face_detection_yunet](https://github.com/opencv/opencv_zoo/tree/master/models/face_detection_yunet)\n",
        "- Scientific Paper: [https://doi.org/10.1007/s11633-023-1423-y](https://doi.org/10.1007/s11633-023-1423-y)"
      ],
      "metadata": {
        "id": "SmxGf0X5LRXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2 \n",
        "\n",
        "## PARAMETERS\n",
        "image = \"./img/object_detection/people2.jpg\"\n",
        "face_detection_model = \"./S3B-dnn_face_detection/face_detection_yunet_2022mar.onnx\" # download from https://github.com/opencv/opencv_zoo/tree/master/models/face_detection_yunet\n",
        "score_threshold = 0.9 # Filtering out faces of score < score_threshold (used to eliminate unlikely faces)\n",
        "nms_threshold = 0.3 # Suppress bounding boxes of iou >= nms_threshold (used to eliminate same bboxes)\n",
        "top_k = 5000 # Keep top_k bounding boxes before NMS.\n",
        "\n",
        "def visualize(input, faces, thickness=2):\n",
        "    if faces is None:\n",
        "        print(\"No face found\")\n",
        "        return\n",
        "    for face in faces:\n",
        "        coords = face[:-1].astype(np.int32) # necessary to convert coordinates to integers before plotting\n",
        "\n",
        "        # draw rectangles of face face\n",
        "        cv2.rectangle(input, (coords[0], coords[1]), (coords[0] + coords[2], coords[1]+coords[3]), (0, 255, 0), thickness)\n",
        "\n",
        "        # draw points of facial features\n",
        "        cv2.circle(input, (coords[4], coords[5]), 2, (255, 0, 0), thickness) # right eye\n",
        "        cv2.circle(input, (coords[6], coords[7]), 2, (0, 0, 255), thickness) # left eye\n",
        "        cv2.circle(input, (coords[8], coords[9]), 2, (0, 255, 0), thickness) # nose tip\n",
        "        cv2.circle(input, (coords[10], coords[11]), 2, (255, 0, 255), thickness) # right corner of mouth\n",
        "        cv2.circle(input, (coords[12], coords[13]), 2, (0, 255, 255), thickness) # left corner of mouth\n",
        "\n",
        "img = cv2.imread(cv2.samples.findFile(image))\n",
        "imgWidth = int(img.shape[1])\n",
        "imgHeight = int(img.shape[0])\n",
        "detector = cv2.FaceDetectorYN.create(face_detection_model, \"\", (imgWidth, imgHeight), score_threshold, nms_threshold, top_k)\n",
        "\n",
        "faces = detector.detect(img)[1]\n",
        "visualize(img, faces)\n",
        "\n",
        "cv2_imshow(img)"
      ],
      "metadata": {
        "id": "LS5EEGrELXFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1\n",
        "\n",
        "Write a code that can detect and **box up** faces (using DNN) and eyes (using Haar Cascade).  \n",
        "Hint 1: *The name of the Haar Cascade is `haarcascade_eye_tree_eyeglasses.xml`*.  \n",
        "Hint 2: *How can you increase the accuracy of the code to detect the eyes?*"
      ],
      "metadata": {
        "id": "WinYHaA0zYKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code goes here"
      ],
      "metadata": {
        "id": "SvG9J0iFzXh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 4: OpenCV GUI and Video"
      ],
      "metadata": {
        "id": "Ob8_i0gRLZ6c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper functions for Section 4 (do not edit!)\n",
        "\n",
        "**Run the following hidden code block.**"
      ],
      "metadata": {
        "id": "OMBvoHoqGvN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import dependencies\n",
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import cv2\n",
        "import numpy as np\n",
        "import PIL\n",
        "import io\n",
        "import html\n",
        "import time\n",
        "\n",
        "# function to convert the JavaScript object into an OpenCV image\n",
        "def js_to_image(js_reply):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          js_reply: JavaScript object containing image from webcam\n",
        "  Returns:\n",
        "          img: OpenCV BGR image\n",
        "  \"\"\"\n",
        "  # decode base64 image\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  # convert bytes to numpy array\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  # decode numpy array into OpenCV BGR image\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "  return img\n",
        "\n",
        "# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n",
        "def bbox_to_bytes(bbox_array):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
        "  Returns:\n",
        "        bytes: Base64 image byte string\n",
        "  \"\"\"\n",
        "  # convert array into PIL image\n",
        "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  # format bbox into png for return\n",
        "  bbox_PIL.save(iobuf, format='png')\n",
        "  # format return string\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "  return bbox_bytes\n",
        "\n",
        "# JavaScript to properly create our live video stream using our webcam as input\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "    \n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "    \n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "    \n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "    \n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "      \n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "           \n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "      \n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML = \n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "      \n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 640; //video.videoWidth;\n",
        "      captureCanvas.height = 480; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "      \n",
        "      return stream;\n",
        "    }\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "      \n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "            \n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "      \n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "      \n",
        "      return {'create': preShow - preCreate, \n",
        "              'show': preCapture - preShow, \n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "  \n",
        "def video_frame(label, bbox):\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  return data"
      ],
      "metadata": {
        "id": "GM-bHtjXpW7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Webcam\n",
        "\n",
        "*You will notice that it is much more difficult on Colab. You are recommended to develop any webcam code using a local machine instead of Colab, if needed.*"
      ],
      "metadata": {
        "id": "P_QgPVBxHAie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# start streaming video from webcam\n",
        "video_stream()\n",
        "# label for video\n",
        "label_html = 'Capturing...'\n",
        "# initialze bounding box to empty\n",
        "bbox = ''\n",
        "count = 0 \n",
        "while True:\n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "    if not js_reply:\n",
        "        break"
      ],
      "metadata": {
        "id": "su-LzuDkxeu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Drawing Shapes"
      ],
      "metadata": {
        "id": "f6VyuUfUxrRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "img = cv2.imread(\"./img/image_processing/orchid.png\")\n",
        "\n",
        "# Drawing rectangle\n",
        "cv2.rectangle(img,(384,0),(510,128),(0,255,0),3)\n",
        "print(\"Rectangle\")\n",
        "cv2_imshow(img)\n",
        "\n",
        "# Drawing line\n",
        "cv2.line(img,(300,50),(400, 50),(255,0,0),3)\n",
        "print(\"Line\")\n",
        "cv2_imshow(img)"
      ],
      "metadata": {
        "id": "Nr70LqbgxtNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GUI Trackbar\n",
        "\n",
        "**Warning!**  \n",
        "Unfortunately, trackbar cannot be used and does not work on Google Colab. \n"
      ],
      "metadata": {
        "id": "aPFMCraDyBSO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2\n",
        "Write a code that can detect faces (using DNN) based off of your webcam feed (not a static image).  \n",
        "Warning: *On google colab, the integration is a quite complicated, so it's ok if it doesn't work!*  \n",
        "Hint: *You can refer to [https://github.com/theAIGuysCode/colab-webcam/blob/main/colab_webcam.ipynb](https://github.com/theAIGuysCode/colab-webcam/blob/main/colab_webcam.ipynb)*"
      ],
      "metadata": {
        "id": "wb1tJOl6ykd_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code goes here"
      ],
      "metadata": {
        "id": "5aHScGAOylya"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}